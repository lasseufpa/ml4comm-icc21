{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "E0NlguFa_iSe",
    "outputId": "f9e53df2-c09d-4cc4-8a60-df1767210c3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...  ./data/lidar_input.npz\n",
      "Reading dataset... ./data/beams_output.npz\n",
      "WARNING:tensorflow:From d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20, 200, 10)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 200, 10)       16910     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 200, 10)       12110     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 6, 40, 10)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 40, 10)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 40, 10)         4910      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 20, 10)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 20, 10)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 20, 10)         910       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 10, 10)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 10, 10)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               153856    \n",
      "=================================================================\n",
      "Total params: 188,696\n",
      "Trainable params: 188,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8955 samples, validate on 2239 samples\n",
      "Epoch 1/2\n",
      "8955/8955 [==============================] - 202s 23ms/sample - loss: 3.2574 - categorical_accuracy: 0.2207 - top_k_categorical_accuracy: 0.5463 - top_10_accuracy: 0.7068 - top_30_accuracy: 0.9131 - top_50_accuracy: 0.9602 - top_100_accuracy: 0.9832 - val_loss: 2.8848 - val_categorical_accuracy: 0.2925 - val_top_k_categorical_accuracy: 0.6396 - val_top_10_accuracy: 0.7852 - val_top_30_accuracy: 0.9527 - val_top_50_accuracy: 0.9795 - val_top_100_accuracy: 0.9924\n",
      "Epoch 2/2\n",
      "8928/8955 [============================>.] - ETA: 0s - loss: 2.7502 - categorical_accuracy: 0.2856 - top_k_categorical_accuracy: 0.6739 - top_10_accuracy: 0.8128 - top_30_accuracy: 0.9640 - top_50_accuracy: 0.9856 - top_100_accuracy: 0.9975"
     ]
    }
   ],
   "source": [
    "#Context: This script uses Raymotime data (https://www.lasse.ufpa.br/raymobtime/) in the context\n",
    "#of the UFPA - ITU Artificial Intelligence/Machine Learning in 5G Challenge (http://ai5gchallenge.ufpa.br/).\n",
    "#Authors       \t\t: Ailton Oliveira, Aldebaro Klautau, Arthur Nascimento, Diego Gomes, Jamelly Ferreira, Walter Frazao\n",
    "#Email          \t: ml5gphy@gmail.com                                          \n",
    "###################################################################\n",
    "'''Trains a deep NN for choosing top-K beams.\n",
    "Note that you need to download the datasets and save them in the folder specified in this code. \n",
    "The default folder name is data.\n",
    "'''\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import model_from_json,Model\n",
    "from tensorflow.keras.layers import Dense,concatenate\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adadelta,Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml4comm_model_handler import ModelHandler\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Support functions\n",
    "###############################################################################\n",
    "\n",
    "#For description about top-k, including the explanation on how they treat ties (which can be misleading\n",
    "#if your classifier is outputting a lot of ties (e.g. all 0's will lead to high top-k)\n",
    "#https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k\n",
    "\n",
    "def top_10_accuracy(y_true,y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true,y_pred,k=10)\n",
    "\n",
    "def top_30_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=30)\n",
    "\n",
    "def top_50_accuracy(y_true,y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true,y_pred,k=50)\n",
    "\n",
    "def top_100_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=100)\n",
    "\n",
    "\n",
    "def sub2ind(array_shape, rows, cols):\n",
    "    ind = rows*array_shape[1] + cols\n",
    "    ind[ind < 0] = -1\n",
    "    ind[ind >= array_shape[0]*array_shape[1]] = -1\n",
    "    return ind\n",
    "\n",
    "def ind2sub(array_shape, ind):\n",
    "    ind[ind < 0] = -1\n",
    "    ind[ind >= array_shape[0]*array_shape[1]] = -1\n",
    "    rows = (ind.astype('int') / array_shape[1])\n",
    "    cols = ind % array_shape[1]\n",
    "    return (rows, cols)\n",
    "\n",
    "def beamsLogScale(y,thresholdBelowMax):\n",
    "        y_shape = y.shape\n",
    "        \n",
    "        for i in range(0,y_shape[0]):            \n",
    "            thisOutputs = y[i,:]\n",
    "            logOut = 20*np.log10(thisOutputs + 1e-30)\n",
    "            minValue = np.amax(logOut) - thresholdBelowMax\n",
    "            zeroedValueIndices = logOut < minValue\n",
    "            thisOutputs[zeroedValueIndices]=0\n",
    "            thisOutputs = thisOutputs / sum(thisOutputs)\n",
    "            y[i,:] = thisOutputs\n",
    "        \n",
    "        return y\n",
    "\n",
    "def getBeamOutput(output_file):\n",
    "    \n",
    "    thresholdBelowMax = 6\n",
    "    \n",
    "    print(\"Reading dataset...\", output_file)\n",
    "    output_cache_file = np.load(output_file)\n",
    "    yMatrix = output_cache_file['output_classification']\n",
    "    \n",
    "    yMatrix = np.abs(yMatrix)\n",
    "    yMatrix /= np.max(yMatrix)\n",
    "    yMatrixShape = yMatrix.shape\n",
    "    num_classes = yMatrix.shape[1] * yMatrix.shape[2]\n",
    "    \n",
    "    y = yMatrix.reshape(yMatrix.shape[0],num_classes)\n",
    "    y = beamsLogScale(y,thresholdBelowMax)\n",
    "    \n",
    "    return y,num_classes\n",
    "\n",
    "###############################################################################\n",
    "# Data configuration\n",
    "###############################################################################\n",
    "tf.device('/device:GPU:0')\n",
    "\n",
    "#Here you can choose a combination of sets of parameters to be used as the neural network input\n",
    "coord = False  #coordinates with the receiver position obtained from GPS\n",
    "img = False  #RGB images obtained from cameras\n",
    "lidar = True  #three-dimensional histogram obtained form LIDAR point cloud\n",
    "\n",
    "num_epochs = 2 #assume a rather small number just to demo things. You should try larger values\n",
    "batch_size = 32\n",
    "tgtRec = 3\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "if coord == True: \n",
    "    #train\n",
    "    coord_train_input_file = '/content/drive/MyDrive/ssp_data/baseline_data/coord_input/coord_input.npz'\n",
    "    coord_train_cache_file = np.load(coord_train_input_file)\n",
    "    X_coord = coord_train_cache_file['coordinates']\n",
    "    X_coord_train, X_coord_validation = train_test_split(X_coord, test_size=0.2, random_state=seed, shuffle=True)\n",
    "\n",
    "    coord_train_input_shape = X_coord_train.shape\n",
    "\n",
    "if img == True:\n",
    "    resizeFac = 20 # Resize Factor\n",
    "    nCh = 1 # The number of channels of the image\n",
    "    imgDim = (360,640) # Image dimensions\n",
    "    method = 1\n",
    "\n",
    "    #train\n",
    "    img_train_input_file = '/content/drive/MyDrive/SSP_data/baseline_data/image_input/img_input_20.npz'\n",
    "    img_train_cache_file = np.load(img_train_input_file)\n",
    "    X_img = img_train_cache_file['inputs']\n",
    "    X_img_train, X_img_validation = train_test_split(X_img, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    print(\"Reading dataset... \",img_train_input_file)\n",
    "\n",
    "    img_train_input_shape = X_img_train.shape\n",
    "\n",
    "if lidar == True:\n",
    "    #train\n",
    "    #lidar_train_input_file = '/content/drive/MyDrive/SSP_data/baseline_data/lidar_input/lidar_input.npz'\n",
    "    lidar_train_input_file = './data/lidar_input.npz'\n",
    "    lidar_train_cache_file = np.load(lidar_train_input_file)\n",
    "    X_lidar = lidar_train_cache_file['input']\n",
    "    X_lidar_train, X_lidar_validation = train_test_split(X_lidar, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    print(\"Reading dataset... \",lidar_train_input_file)\n",
    "    lidar_train_input_shape = X_lidar_train.shape\n",
    "\n",
    "###############################################################################\n",
    "# Output configuration\n",
    "#train\n",
    "output_file = './data/beams_output.npz'\n",
    "y_output,num_classes = getBeamOutput(output_file)\n",
    "y_train, y_validation = train_test_split(y_output, test_size=0.2, random_state=seed, shuffle=True)\n",
    "\n",
    "##############################################################################\n",
    "# Model configuration\n",
    "##############################################################################\n",
    "\n",
    "#multimodal\n",
    "multimodal = [coord, img, lidar]\n",
    "plot = True # Active Plot output\n",
    "\n",
    "#validationFraction = 0.2 #from 0 to 1\n",
    "modelHand = ModelHandler()\n",
    "opt = Adam()\n",
    "\n",
    "if coord:\n",
    "    coord_model = modelHand.createArchitecture('coord_mlp',num_classes,coord_train_input_shape[1],'complete')\n",
    "if img:\n",
    "    if nCh==1:   \n",
    "        img_model = modelHand.createArchitecture('light_image',num_classes,[img_train_input_shape[1],img_train_input_shape[2],1],'complete')\n",
    "    else:\n",
    "        img_model = modelHand.createArchitecture('light_image',num_classes,[img_train_input_shape[1],img_train_input_shape[2],img_train_input_shape[3]],'complete')\n",
    "if lidar:\n",
    "    lidar_model = modelHand.createArchitecture('lidar_simple',num_classes,[lidar_train_input_shape[1],lidar_train_input_shape[2],lidar_train_input_shape[3]],'complete')\n",
    "\n",
    "if sum(multimodal) == 2:\n",
    "    if coord and lidar:\n",
    "        combined_model = concatenate([coord_model.output,lidar_model.output])\n",
    "        z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "        model = Model(inputs=[coord_model.input,lidar_model.input],outputs=z)\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit([X_coord_train,X_lidar_train],y_train, \n",
    "        validation_data=([X_coord_validation, X_lidar_validation], y_validation),epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "    elif coord and img:\n",
    "        combined_model = concatenate([coord_model.output,img_model.output])\n",
    "        z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "        model = Model(inputs=[coord_model.input,img_model.input],outputs=z)\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit([X_coord_train,X_img_train],y_train,\n",
    "        validation_data=([X_coord_validation, X_img_validation], y_validation), epochs=num_epochs,batch_size=batch_size)\n",
    "    \n",
    "    else:\n",
    "        combined_model = concatenate([lidar_model.output,img_model.output])\n",
    "        z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "        model = Model(inputs=[lidar_model.input,img_model.input],outputs=z)\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit([X_lidar_train,X_img_train],y_train, \n",
    "        validation_data=([X_lidar_validation, X_img_validation], y_validation), epochs=num_epochs,batch_size=batch_size)\n",
    "elif sum(multimodal) == 3:\n",
    "    combined_model = concatenate([lidar_model.output,img_model.output, coord_model.output])\n",
    "    z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "    model = Model(inputs=[lidar_model.input,img_model.input, coord_model.input],outputs=z)\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                optimizer=opt,\n",
    "                metrics=[metrics.categorical_accuracy,\n",
    "                        metrics.top_k_categorical_accuracy,\n",
    "                        top_10_accuracy,\n",
    "                        top_30_accuracy,\n",
    "                        top_50_accuracy,\n",
    "                        top_100_accuracy])\n",
    "    model.summary()\n",
    "    hist = model.fit([X_lidar_train,X_img_train,X_coord_train],y_train,\n",
    "            validation_data=([X_lidar_validation, X_img_validation, X_coord_validation], y_validation),\n",
    "            epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "else:\n",
    "    if coord:\n",
    "        model = coord_model\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                            optimizer=opt,\n",
    "                            metrics=[metrics.categorical_accuracy,\n",
    "                                    metrics.top_k_categorical_accuracy,\n",
    "                                    top_10_accuracy,\n",
    "                                    top_30_accuracy, \n",
    "                                    top_50_accuracy,\n",
    "                                    top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit(X_coord_train,y_train, \n",
    "        validation_data=(X_coord_validation, y_validation),epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "    elif img:\n",
    "        model = img_model  \n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit(X_img_train,y_train, \n",
    "        validation_data=(X_img_validation, y_validation),epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "    else:\n",
    "        model = lidar_model\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit(X_lidar_train,y_train,epochs=num_epochs,batch_size=batch_size, validation_data=(X_lidar_validation, y_validation))\n",
    "\n",
    "with open('history.txt', 'w') as f: \n",
    "       f.write(str(hist.history))\n",
    "\n",
    "if plot:\n",
    "       import matplotlib.pyplot as plt\n",
    "       import numpy as np\n",
    "\n",
    "       k_beams = [1, 5, 10, 30, 50]\n",
    "       fig, ax = plt.subplots(figsize=(7,4))\n",
    "       # acurracy's\n",
    "       y = [max(hist.history['categorical_accuracy']),\n",
    "       max(hist.history['top_k_categorical_accuracy']),\n",
    "       max(hist.history['top_10_accuracy']),\n",
    "       max(hist.history['top_30_accuracy']),\n",
    "       max(hist.history['top_50_accuracy'])]\n",
    "       ax.plot(k_beams,y, 'r--s', label = 'Beam selection accuracy')\n",
    "       # original labels\n",
    "       '''ax.plot(k_beams,y_ori, 'b--s', label = 'Correct orientation-LIDAR')\n",
    "       ax.plot(k_beams,y_sori, 'k--s', label = 'Fixed oientation-LIDAR ')\n",
    "       ax.plot(k_beams,y_MM_ori, 'm--s', label = 'Correct orientation-MM')\n",
    "       ax.plot(k_beams,y_MM_sori, 'r--s', label = 'Fixed oientation-MM')'''\n",
    "       ax.set(xlabel='Top K', ylabel='Accuracy')\n",
    "       plt.xlim(right=51)\n",
    "       ax.grid()\n",
    "       plt.legend()\n",
    "       plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySgwKqw3DPSh"
   },
   "source": [
    "#Accuracy / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "trfsmDjXl8hl",
    "outputId": "fd854dbb-ea86-4aac-dc93-7726e2609071"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.plot(hist.history['categorical_accuracy'], 'b', label = 'Categorical accuracy', linewidth=2)\n",
    "ax.plot(hist.history['top_k_categorical_accuracy'], 'k--', label = 'Top 5', linewidth=2)\n",
    "ax.plot(hist.history['top_10_accuracy'], 'm', label = 'Top 10', linewidth=2)\n",
    "ax.plot(hist.history['top_30_accuracy'], 'g--', label = 'Top 30', linewidth=2)\n",
    "ax.plot(hist.history['top_50_accuracy'], 'r', label = 'Top 50', linewidth=2)\n",
    "ax.set(xlabel='Epochs', ylabel='Accuracy')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zeUfHR3DH7h"
   },
   "source": [
    "# Accuracy / Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQWanItzcLI_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def top_n_Rt(y_predict,valid_, N = 1):\n",
    "    predict = y_predict[:,:N]\n",
    "    original= valid_[:,-N:]\n",
    "    rt = np.sum(1+predict) / np.sum(1+original)\n",
    "    return rt\n",
    "\n",
    "y_predict = model.predict(X_lidar_validation)\n",
    "valid_ = np.sort(y_validation)\n",
    "\n",
    "y = [max(hist.history['categorical_accuracy']),\n",
    "     max(hist.history['top_k_categorical_accuracy']),\n",
    "     max(hist.history['top_10_accuracy']),\n",
    "     max(hist.history['top_30_accuracy']),\n",
    "     max(hist.history['top_50_accuracy'])]\n",
    "y_rt=[]\n",
    "k_beams = [1, 5, 10, 30, 50]\n",
    "for i in k_beams:\n",
    "    rt = top_n_Rt(y_predict,valid_,i)\n",
    "    y_rt.append(rt)\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.plot(k_beams,y_rt, 'b', label = 'Throughput ratio')\n",
    "ax.plot(k_beams,y, 'r', label = 'Beam selection accuracy')\n",
    "ax.set(xlabel='Top K', ylabel='Accuracy')\n",
    "plt.xlim(right=51)\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yoP7rk9DAqE"
   },
   "source": [
    "# Throughput ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu190Nesmkp6"
   },
   "outputs": [],
   "source": [
    "def top_n_Rt(y_predict,valid_, N = 1):\n",
    "    predict = y_predict[:,:N]\n",
    "    original= valid_[:,-N:]\n",
    "    rt = np.sum(1+predict) / np.sum(1+original)\n",
    "    return rt\n",
    "\n",
    "y_predict = model.predict(X_lidar_validation)\n",
    "valid_ = np.sort(y_validation)\n",
    "examples = valid_.shape[0]\n",
    "y=[]\n",
    "for i in range(examples):\n",
    "    rt = top_n_Rt(y_predict,valid_,i)\n",
    "    y.append(rt)\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.plot(y[:], 'b--', label = 'Throughput ratio')\n",
    "ax.set(xlabel='Top K', ylabel='Accuracy')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFBQbre7CuNW"
   },
   "source": [
    "# Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ay33Ma6kfM2T"
   },
   "outputs": [],
   "source": [
    "fileNameIdentifier = 'training_history'\n",
    "f = open(fileNameIdentifier + '.txt','w')\n",
    "f.write(str(hist.history))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "beam_selection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
