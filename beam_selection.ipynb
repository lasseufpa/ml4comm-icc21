{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "E0NlguFa_iSe",
    "outputId": "f9e53df2-c09d-4cc4-8a60-df1767210c3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...  ./data/lidar_input.npz\n",
      "Reading dataset... ./data/beams_output.npz\n",
      "WARNING:tensorflow:From d:\\Programs\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20, 200, 10)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 200, 10)       16910     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 200, 30)       36330     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 200, 25)       60775     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 10, 200, 25)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 200, 25)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 200, 20)       24520     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 100, 20)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 100, 15)       7515      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 100, 15)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 10, 100, 10)       1360      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 100, 1)        11        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               256256    \n",
      "=================================================================\n",
      "Total params: 403,677\n",
      "Trainable params: 403,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8955 samples, validate on 2239 samples\n",
      "Epoch 1/10\n",
      " 352/8955 [>.............................] - ETA: 8:10 - loss: 4.4614 - categorical_accuracy: 0.1364 - top_k_categorical_accuracy: 0.3551 - top_10_accuracy: 0.5000 - top_30_accuracy: 0.7386 - top_50_accuracy: 0.8324 - top_100_accuracy: 0.9034"
     ]
    }
   ],
   "source": [
    "#Note that you need to download the datasets and save them in the folder specified in this code. \n",
    "#The default folder name is data.\n",
    "#\n",
    "#Script context use\t: This script uses Raymotime data (https://www.lasse.ufpa.br/raymobtime/) in the context of the UFPA - ITU Artificial Intelligence/Machine Learning in 5G Challenge (http://ai5gchallenge.ufpa.br/).\n",
    "#Author       \t\t: Ailton Oliveira, Aldebaro Klautau, Arthur Nascimento, Diego Gomes, Jamelly Ferreira, Walter Frazao\n",
    "#Email          \t: ml5gphy@gmail.com                                          \n",
    "#License\t\t: This script is distributed under \"Public Domain\" license.\n",
    "###################################################################\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''Trains a deep NN for choosing top-K beams\n",
    "Adapted by AK: Aug 7, 2018\n",
    "See\n",
    "https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "and\n",
    "https://stackoverflow.com/questions/45642077/do-i-need-to-use-one-hot-encoding-if-my-output-variable-is-binary\n",
    "See for explanation about convnet and filters:\n",
    "https://datascience.stackexchange.com/questions/16463/what-is-are-the-default-filters-used-by-keras-convolution2d\n",
    "and\n",
    "http://cs231n.github.io/convolutional-networks/\n",
    "'''\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import model_from_json,Model\n",
    "from tensorflow.keras.layers import Dense,concatenate\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adadelta,Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml4comm_model_handler import ModelHandler\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Support functions\n",
    "###############################################################################\n",
    "\n",
    "#For description about top-k, including the explanation on how they treat ties (which can be misleading\n",
    "#if your classifier is outputting a lot of ties (e.g. all 0's will lead to high top-k)\n",
    "#https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k\n",
    "\n",
    "def top_10_accuracy(y_true,y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true,y_pred,k=10)\n",
    "\n",
    "def top_30_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=30)\n",
    "\n",
    "def top_50_accuracy(y_true,y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true,y_pred,k=50)\n",
    "\n",
    "def top_100_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=100)\n",
    "\n",
    "\n",
    "def sub2ind(array_shape, rows, cols):\n",
    "    ind = rows*array_shape[1] + cols\n",
    "    ind[ind < 0] = -1\n",
    "    ind[ind >= array_shape[0]*array_shape[1]] = -1\n",
    "    return ind\n",
    "\n",
    "def ind2sub(array_shape, ind):\n",
    "    ind[ind < 0] = -1\n",
    "    ind[ind >= array_shape[0]*array_shape[1]] = -1\n",
    "    rows = (ind.astype('int') / array_shape[1])\n",
    "    cols = ind % array_shape[1]\n",
    "    return (rows, cols)\n",
    "\n",
    "def beamsLogScale(y,thresholdBelowMax):\n",
    "        y_shape = y.shape\n",
    "        \n",
    "        for i in range(0,y_shape[0]):            \n",
    "            thisOutputs = y[i,:]\n",
    "            logOut = 20*np.log10(thisOutputs + 1e-30)\n",
    "            minValue = np.amax(logOut) - thresholdBelowMax\n",
    "            zeroedValueIndices = logOut < minValue\n",
    "            thisOutputs[zeroedValueIndices]=0\n",
    "            thisOutputs = thisOutputs / sum(thisOutputs)\n",
    "            y[i,:] = thisOutputs\n",
    "        \n",
    "        return y\n",
    "\n",
    "def getBeamOutput(output_file):\n",
    "    \n",
    "    thresholdBelowMax = 6\n",
    "    \n",
    "    print(\"Reading dataset...\", output_file)\n",
    "    output_cache_file = np.load(output_file)\n",
    "    yMatrix = output_cache_file['output_classification']\n",
    "    \n",
    "    yMatrix = np.abs(yMatrix)\n",
    "    yMatrix /= np.max(yMatrix)\n",
    "    yMatrixShape = yMatrix.shape\n",
    "    num_classes = yMatrix.shape[1] * yMatrix.shape[2]\n",
    "    \n",
    "    y = yMatrix.reshape(yMatrix.shape[0],num_classes)\n",
    "    y = beamsLogScale(y,thresholdBelowMax)\n",
    "    \n",
    "    return y,num_classes\n",
    "\n",
    "###############################################################################\n",
    "# Data configuration\n",
    "###############################################################################\n",
    "tf.device('/device:GPU:0')\n",
    "\n",
    "#Here you can choose a combination of sets of parameters to be used as the neural network input\n",
    "coord = False  #coordinates with the receiver position obtained from GPS\n",
    "img = False  #RGB images obtained from cameras\n",
    "lidar = True  #three-dimensional histogram obtained form LIDAR point cloud\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "tgtRec = 3\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "if coord == True: \n",
    "    #train\n",
    "    coord_train_input_file = '/content/drive/MyDrive/ssp_data/baseline_data/coord_input/coord_input.npz'\n",
    "    coord_train_cache_file = np.load(coord_train_input_file)\n",
    "    X_coord = coord_train_cache_file['coordinates']\n",
    "    X_coord_train, X_coord_validation = train_test_split(X_coord, test_size=0.2, random_state=seed, shuffle=True)\n",
    "\n",
    "    coord_train_input_shape = X_coord_train.shape\n",
    "\n",
    "if img == True:\n",
    "    resizeFac = 20 # Resize Factor\n",
    "    nCh = 1 # The number of channels of the image\n",
    "    imgDim = (360,640) # Image dimensions\n",
    "    method = 1\n",
    "\n",
    "    #train\n",
    "    img_train_input_file = '/content/drive/MyDrive/SSP_data/baseline_data/image_input/img_input_20.npz'\n",
    "    img_train_cache_file = np.load(img_train_input_file)\n",
    "    X_img = img_train_cache_file['inputs']\n",
    "    X_img_train, X_img_validation = train_test_split(X_img, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    print(\"Reading dataset... \",img_train_input_file)\n",
    "\n",
    "    img_train_input_shape = X_img_train.shape\n",
    "\n",
    "if lidar == True:\n",
    "    #train\n",
    "    #lidar_train_input_file = '/content/drive/MyDrive/SSP_data/baseline_data/lidar_input/lidar_input.npz'\n",
    "    lidar_train_input_file = './data/lidar_input.npz'\n",
    "    lidar_train_cache_file = np.load(lidar_train_input_file)\n",
    "    X_lidar = lidar_train_cache_file['input']\n",
    "    X_lidar_train, X_lidar_validation = train_test_split(X_lidar, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    print(\"Reading dataset... \",lidar_train_input_file)\n",
    "    lidar_train_input_shape = X_lidar_train.shape\n",
    "\n",
    "###############################################################################\n",
    "# Output configuration\n",
    "#train\n",
    "output_file = './data/beams_output.npz'\n",
    "y_output,num_classes = getBeamOutput(output_file)\n",
    "y_train, y_validation = train_test_split(y_output, test_size=0.2, random_state=seed, shuffle=True)\n",
    "\n",
    "##############################################################################\n",
    "# Model configuration\n",
    "##############################################################################\n",
    "\n",
    "#multimodal\n",
    "multimodal = [coord, img, lidar]\n",
    "plot = True # Active Plot output\n",
    "\n",
    "#validationFraction = 0.2 #from 0 to 1\n",
    "modelHand = ModelHandler()\n",
    "opt = Adam()\n",
    "\n",
    "if coord:\n",
    "    coord_model = modelHand.createArchitecture('coord_mlp',num_classes,coord_train_input_shape[1],'complete')\n",
    "if img:\n",
    "    if nCh==1:   \n",
    "        img_model = modelHand.createArchitecture('light_image',num_classes,[img_train_input_shape[1],img_train_input_shape[2],1],'complete')\n",
    "    else:\n",
    "        img_model = modelHand.createArchitecture('light_image',num_classes,[img_train_input_shape[1],img_train_input_shape[2],img_train_input_shape[3]],'complete')\n",
    "if lidar:\n",
    "    lidar_model = modelHand.createArchitecture('lidar_marcus',num_classes,[lidar_train_input_shape[1],lidar_train_input_shape[2],lidar_train_input_shape[3]],'complete')\n",
    "\n",
    "if sum(multimodal) == 2:\n",
    "    if coord and lidar:\n",
    "        combined_model = concatenate([coord_model.output,lidar_model.output])\n",
    "        z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "        model = Model(inputs=[coord_model.input,lidar_model.input],outputs=z)\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit([X_coord_train,X_lidar_train],y_train, \n",
    "        validation_data=([X_coord_validation, X_lidar_validation], y_validation),epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "    elif coord and img:\n",
    "        combined_model = concatenate([coord_model.output,img_model.output])\n",
    "        z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "        model = Model(inputs=[coord_model.input,img_model.input],outputs=z)\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit([X_coord_train,X_img_train],y_train,\n",
    "        validation_data=([X_coord_validation, X_img_validation], y_validation), epochs=num_epochs,batch_size=batch_size)\n",
    "    \n",
    "    else:\n",
    "        combined_model = concatenate([lidar_model.output,img_model.output])\n",
    "        z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "        model = Model(inputs=[lidar_model.input,img_model.input],outputs=z)\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit([X_lidar_train,X_img_train],y_train, \n",
    "        validation_data=([X_lidar_validation, X_img_validation], y_validation), epochs=num_epochs,batch_size=batch_size)\n",
    "elif sum(multimodal) == 3:\n",
    "    combined_model = concatenate([lidar_model.output,img_model.output, coord_model.output])\n",
    "    z = Dense(num_classes,activation=\"relu\")(combined_model)\n",
    "    model = Model(inputs=[lidar_model.input,img_model.input, coord_model.input],outputs=z)\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                optimizer=opt,\n",
    "                metrics=[metrics.categorical_accuracy,\n",
    "                        metrics.top_k_categorical_accuracy,\n",
    "                        top_10_accuracy,\n",
    "                        top_30_accuracy,\n",
    "                        top_50_accuracy,\n",
    "                        top_100_accuracy])\n",
    "    model.summary()\n",
    "    hist = model.fit([X_lidar_train,X_img_train,X_coord_train],y_train,\n",
    "            validation_data=([X_lidar_validation, X_img_validation, X_coord_validation], y_validation),\n",
    "            epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "else:\n",
    "    if coord:\n",
    "        model = coord_model\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                            optimizer=opt,\n",
    "                            metrics=[metrics.categorical_accuracy,\n",
    "                                    metrics.top_k_categorical_accuracy,\n",
    "                                    top_10_accuracy,\n",
    "                                    top_30_accuracy, \n",
    "                                    top_50_accuracy,\n",
    "                                    top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit(X_coord_train,y_train, \n",
    "        validation_data=(X_coord_validation, y_validation),epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "    elif img:\n",
    "        model = img_model  \n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit(X_img_train,y_train, \n",
    "        validation_data=(X_img_validation, y_validation),epochs=num_epochs,batch_size=batch_size)\n",
    "\n",
    "    else:\n",
    "        model = lidar_model\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[metrics.categorical_accuracy,\n",
    "                            metrics.top_k_categorical_accuracy,\n",
    "                            top_10_accuracy,\n",
    "                            top_30_accuracy,\n",
    "                            top_50_accuracy,\n",
    "                            top_100_accuracy])\n",
    "        model.summary()\n",
    "        hist = model.fit(X_lidar_train,y_train,epochs=num_epochs,batch_size=batch_size, validation_data=(X_lidar_validation, y_validation))\n",
    "\n",
    "with open('history.txt', 'w') as f: \n",
    "       f.write(str(history.history))\n",
    "\n",
    "if plot:\n",
    "       import matplotlib.pyplot as plt\n",
    "       import numpy as np\n",
    "\n",
    "       k_beams = [1, 5, 10, 30, 50]\n",
    "       fig, ax = plt.subplots(figsize=(7,4))\n",
    "       # acurracy's\n",
    "       y = [max(hist.history['categorical_accuracy']),\n",
    "       max(hist.history['top_k_categorical_accuracy']),\n",
    "       max(hist.history['top_10_accuracy']),\n",
    "       max(hist.history['top_30_accuracy']),\n",
    "       max(hist.history['top_50_accuracy'])]\n",
    "       ax.plot(k_beams,y, 'r--s', label = 'Beam selection accuracy')\n",
    "       # original labels\n",
    "       '''ax.plot(k_beams,y_ori, 'b--s', label = 'Correct orientation-LIDAR')\n",
    "       ax.plot(k_beams,y_sori, 'k--s', label = 'Fixed oientation-LIDAR ')\n",
    "       ax.plot(k_beams,y_MM_ori, 'm--s', label = 'Correct orientation-MM')\n",
    "       ax.plot(k_beams,y_MM_sori, 'r--s', label = 'Fixed oientation-MM')'''\n",
    "       ax.set(xlabel='Top K', ylabel='Accuracy')\n",
    "       plt.xlim(right=51)\n",
    "       ax.grid()\n",
    "       plt.legend()\n",
    "       plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySgwKqw3DPSh"
   },
   "source": [
    "#Accuracy / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "trfsmDjXl8hl",
    "outputId": "fd854dbb-ea86-4aac-dc93-7726e2609071"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.plot(hist.history['categorical_accuracy'], 'b', label = 'Categorical accuracy', linewidth=2)\n",
    "ax.plot(hist.history['top_k_categorical_accuracy'], 'k--', label = 'Top 5', linewidth=2)\n",
    "ax.plot(hist.history['top_10_accuracy'], 'm', label = 'Top 10', linewidth=2)\n",
    "ax.plot(hist.history['top_30_accuracy'], 'g--', label = 'Top 30', linewidth=2)\n",
    "ax.plot(hist.history['top_50_accuracy'], 'r', label = 'Top 50', linewidth=2)\n",
    "ax.set(xlabel='Epochs', ylabel='Accuracy')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zeUfHR3DH7h"
   },
   "source": [
    "# Accuracy / Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQWanItzcLI_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def top_n_Rt(y_predict,valid_, N = 1):\n",
    "    predict = y_predict[:,:N]\n",
    "    original= valid_[:,-N:]\n",
    "    rt = np.sum(1+predict) / np.sum(1+original)\n",
    "    return rt\n",
    "\n",
    "y_predict = model.predict(X_lidar_validation)\n",
    "valid_ = np.sort(y_validation)\n",
    "\n",
    "y = [max(hist.history['categorical_accuracy']),\n",
    "     max(hist.history['top_k_categorical_accuracy']),\n",
    "     max(hist.history['top_10_accuracy']),\n",
    "     max(hist.history['top_30_accuracy']),\n",
    "     max(hist.history['top_50_accuracy'])]\n",
    "y_rt=[]\n",
    "k_beams = [1, 5, 10, 30, 50]\n",
    "for i in k_beams:\n",
    "    rt = top_n_Rt(y_predict,valid_,i)\n",
    "    y_rt.append(rt)\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.plot(k_beams,y_rt, 'b', label = 'Throughput ratio')\n",
    "ax.plot(k_beams,y, 'r', label = 'Beam selection accuracy')\n",
    "ax.set(xlabel='Top K', ylabel='Accuracy')\n",
    "plt.xlim(right=51)\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yoP7rk9DAqE"
   },
   "source": [
    "# Throughput ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu190Nesmkp6"
   },
   "outputs": [],
   "source": [
    "def top_n_Rt(y_predict,valid_, N = 1):\n",
    "    predict = y_predict[:,:N]\n",
    "    original= valid_[:,-N:]\n",
    "    rt = np.sum(1+predict) / np.sum(1+original)\n",
    "    return rt\n",
    "\n",
    "y_predict = model.predict(X_lidar_validation)\n",
    "valid_ = np.sort(y_validation)\n",
    "examples = valid_.shape[0]\n",
    "y=[]\n",
    "for i in range(examples):\n",
    "    rt = top_n_Rt(y_predict,valid_,i)\n",
    "    y.append(rt)\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.plot(y[50:], 'b--', label = 'Throughput ratio')\n",
    "ax.set(xlabel='Top K', ylabel='Accuracy')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LW_0LGuC238"
   },
   "source": [
    "# SSP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djk4LHjYbBgK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "k_beams = [1, 5, 10, 30, 50]\n",
    "#plt.figure(figsize=(7,2.5))\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "y_ori = [0.46097153425216675, 0.7924064993858337 ,0.8811836838722229, 0.9738693237304688, 0.9891680479049683]\n",
    "y_sori = [0.4347291886806488, 0.7599106431007385, 0.8573980927467346, 0.9615857005119324, 0.9877163767814636]\n",
    "y_MM_ori = [0.2361, 0.5492, 0.6675, 1.0000, 1.0000]\n",
    "y_MM_sori = [0.1020, 0.3373, 0.4610, 1.0000, 1.0000]\n",
    "ax.plot(k_beams,y_ori, 'b--s', label = 'Correct orientation-LIDAR')\n",
    "ax.plot(k_beams,y_sori, 'k--s', label = 'Fixed oientation-LIDAR ')\n",
    "ax.plot(k_beams,y_MM_ori, 'm--s', label = 'Correct orientation-MM')\n",
    "ax.plot(k_beams,y_MM_sori, 'r--s', label = 'Fixed oientation-MM')\n",
    "ax.set(xlabel='Top K', ylabel='Accuracy')\n",
    "plt.xlim(right=51)\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFBQbre7CuNW"
   },
   "source": [
    "# Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ay33Ma6kfM2T"
   },
   "outputs": [],
   "source": [
    "fileNameIdentifier = '/content/drive/MyDrive/SSP_data/baseline_data/obstacles'\n",
    "f = open(fileNameIdentifier + '.txt','w')\n",
    "f.write(str(hist.history))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "beam_selection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
